{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c5f1d1-db55-4540-9c49-a210df07319f",
   "metadata": {},
   "source": [
    "<center><h1>PantryPalette: Your Ingredient-Inspired Recipe Guide</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96852a-3d1c-4025-b840-0216c111c5ae",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c01efc-ac5f-4663-ac93-26faa45cadd5",
   "metadata": {},
   "source": [
    "Every day, households struggle with unused ingredients in their kitchens, often leading to food waste due\n",
    "to a lack of inspiration or knowledge on how to use them effectively. Many individuals resort to takeout\n",
    "or repetitive meals, simply because they don‚Äôt know what to cook with the ingredients they already have.\n",
    "\n",
    "\n",
    "Meanwhile, traditional recipe search methods require manual browsing through extensive databases,\n",
    "leading to time-consuming and frustrating experiences. Users with dietary restrictions, allergies, or\n",
    "cultural food preferences face an additional challenge in finding recipes that align with their needs.\n",
    "\n",
    "PantryPalette is designed to solve this problem by offering a seamless, ingredient-based recipe discovery\n",
    "experience that helps users maximize their groceries, reduce food waste, and discover new meal ideas\n",
    "with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eede9b6",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adf9de",
   "metadata": {},
   "source": [
    "The purpose of the MLOps project is to develop an end-to-end data science solution that is usable by someone with no technical knowledge. \n",
    "\n",
    "It must include the following components\n",
    "- A process map built which documents the process\n",
    "- Data ingested from an online source\n",
    "- A data repository and model repository\n",
    "- A predictive model built off the data\n",
    "- The model predictions\n",
    "\n",
    "  a. Put into a Streamlit application\n",
    "\n",
    "  b. Via a Docker deployment\n",
    "\n",
    "  c. Which is accessible to users\n",
    "  \n",
    "- A model monitoring dashboard built\n",
    "- Documentation for the model process and risks with the production\n",
    "\n",
    "The purpose is not to have a complex model. It is meant to have a simpler model that works! All of this is to be done in teams of 2, along with individual users. Both the modeling teams and users will take part in the presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc49dbf-028f-4eca-8dfe-d6d7248d171c",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cd67b-f867-4d84-842c-2ac1282c5496",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD;\n",
    "    A[User Inputs Ingredients] --> B[Preprocessing]\n",
    "    B --> C[Tokenization & Standardization]\n",
    "    C --> D[Feature Engineering]\n",
    "    D --> E[TF-IDF Vectorization]\n",
    "    E --> F[Cosine Similarity Calculation]\n",
    "    F --> G[Retrieve Relevant Recipes]\n",
    "    G --> H[Rank Recipes Based on Similarity]\n",
    "    H --> I[Provide Alternative Ingredient Suggestions]\n",
    "    I --> J[Display Recipes with Ingredients & Instructions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912886b7-d084-4964-820e-78328eadc0f1",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0044d-f7c5-4eca-88c5-c3fc48640870",
   "metadata": {},
   "source": [
    "## Data Collection and Ingestion\n",
    "\n",
    "- Get RecipeNLG data (static data) + Spoonacular API (dynamic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6086d10-2a06-49f1-9e2b-8d0aa6d45d7d",
   "metadata": {},
   "source": [
    "### Static Data Collection (RecipeNLG Dataset)\n",
    "\n",
    "- Get RecipeNLG data (static data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipenlg = pd.read_csv(\"../dataset/RecipeNLG_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49877b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipenlg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b8636-7baa-4d34-9cbc-2237bea9c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipenlg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61acc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receipenlg = receipenlg.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "# receipenlg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipenlg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "receipenlg.drop(columns=[\"Unnamed: 0\", \"link\", \"source\", \"NER\"], inplace=True)\n",
    "receipenlg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static = receipenlg.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c78fb4-cb9c-47e9-bcf1-976086d9c0a2",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "-  (dynamic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a042cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dynamic = pd.read_csv(\"../dataset/recipes.csv\")\n",
    "df_dynamic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df_dynamic.drop(columns=[\"image\", \"total time\", \"description\"], inplace=True)\n",
    "df_dynamic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418161fb-b3df-43a5-ab8d-4c64062711c6",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- Clean & standardize ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d6d46",
   "metadata": {},
   "source": [
    "Used regex and string methods to clean and standardize:\n",
    "- Ingredients: lowercase, alphanumeric filtering, list normalization.\n",
    "- Instructions: sentence splitting, stripping, and cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# -----------------------\n",
    "# Step 1: Standardize Columns\n",
    "# -----------------------\n",
    "df_static.rename(columns={'directions': 'instructions'}, inplace=True)\n",
    "df_dynamic.rename(columns={'Recipe_name': 'title',\n",
    "                           'Recipe_ingredients': 'ingredients',\n",
    "                           'Recipe_instructions': 'instructions'}, inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# Step 2: Clean Ingredients\n",
    "# -----------------------\n",
    "def process_ingredients(ing):\n",
    "    if isinstance(ing, list):\n",
    "        return [re.sub(r'[^a-z0-9\\s.]', '', i.lower().strip()) for i in ing]\n",
    "    elif isinstance(ing, str):\n",
    "        tokens = ing.split()\n",
    "        return [re.sub(r'[^a-z0-9\\s.]', '', i.lower().strip()) for i in tokens if len(i) > 1]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "df_static['ingredients'] = df_static['ingredients'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_dynamic['ingredients'] = df_dynamic['ingredients'].apply(process_ingredients)\n",
    "\n",
    "# -----------------------\n",
    "# Step 3: Clean Instructions\n",
    "# -----------------------\n",
    "def process_instructions(instr):\n",
    "    if isinstance(instr, list):\n",
    "        return [i.strip() for i in instr if len(i.strip()) > 0]\n",
    "    elif isinstance(instr, str):\n",
    "        return [i.strip() for i in instr.split('.') if len(i.strip()) > 0]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "df_static['instructions'] = df_static['instructions'].apply(process_instructions)\n",
    "df_dynamic['instructions'] = df_dynamic['instructions'].apply(process_instructions)\n",
    "\n",
    "# -----------------------\n",
    "# Step 4: Merge Datasets\n",
    "# -----------------------\n",
    "df_combined = pd.concat([df_static, df_dynamic], ignore_index=True)\n",
    "df_combined.dropna(subset=['ingredients', 'instructions'], inplace=True)\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dcff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../processed_dataset\", exist_ok=True)\n",
    "\n",
    "# Save clean dataset\n",
    "df_combined.to_csv(\"../processed_dataset/combined_recipes_cleaned.csv\", index=False)\n",
    "print(\"Data integration complete. Ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your combined dataset\n",
    "df = pd.read_csv(\"../processed_dataset/combined_recipes_cleaned.csv\")  # Replace with actual path\n",
    "\n",
    "# STEP 1: Parse ingredients column into token list\n",
    "def flatten_and_tokenize(ingredient_col):\n",
    "    all_tokens = []\n",
    "    for entry in ingredient_col:\n",
    "        try:\n",
    "            ingredients = ast.literal_eval(entry) if isinstance(entry, str) and entry.startswith(\"[\") else [entry]\n",
    "            for line in ingredients:\n",
    "                # Lowercase, remove punctuation/numbers\n",
    "                line = line.lower()\n",
    "                line = re.sub(r'[^a-z\\s]', '', line)\n",
    "                tokens = line.split()\n",
    "                all_tokens.extend(tokens)\n",
    "        except:\n",
    "            continue\n",
    "    return all_tokens\n",
    "\n",
    "# STEP 2: Generate token frequency\n",
    "tokens = flatten_and_tokenize(df['ingredients'])\n",
    "token_freq = Counter(tokens)\n",
    "\n",
    "# STEP 3: Convert to DataFrame for inspection\n",
    "stopword_df = pd.DataFrame(token_freq.items(), columns=[\"token\", \"count\"]).sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# STEP 4: Show top 100 most common tokens\n",
    "print(stopword_df.head(100))\n",
    "\n",
    "# Optional: Save all tokens with counts\n",
    "stopword_df.to_csv(\"../processed_dataset/ingredient_token_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6051241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define custom stopwords \n",
    "custom_stopwords = stop_words.union({\n",
    "    # Measurement units\n",
    "    'c', 'cup', 'cups', 'tsp', 'teaspoon', 'teaspoons', 'tbsp', 'tablespoon', 'tablespoons',\n",
    "    'oz', 'ounce', 'ounces', 'lb', 'lbs', 'pound', 'pounds', 'g', 'gram', 'grams', 'kg',\n",
    "    'ml', 'milliliter', 'milliliters', 'l', 'liter', 'liters', 'qt', 'quart', 'quarts',\n",
    "    'pt', 'pint', 'pints', 'gal', 'gallon', 'gallons', 'pkg', 'pkgs', 'package', 'packages',\n",
    "    'stick', 'sticks', 'dash', 'pinch', 'can', 'cans', 'fluid', 'fl', 'jar', 'jars',\n",
    "    'box', 'boxes', 'bottle', 'bottles', 't', 'tbs', 'tbls', 'qt.', 'pt.', 'oz.', 'lb.', 'g.', 'ml.', 'kg.', 'l.', 'pkg.', 'pkt',\n",
    "\n",
    "    # Preparation and cooking descriptors\n",
    "    'chopped', 'minced', 'diced', 'sliced', 'grated', 'crushed', 'shredded', 'cut',\n",
    "    'peeled', 'optional', 'seeded', 'halved', 'coarsely', 'finely', 'thinly', 'roughly',\n",
    "    'cubed', 'crumbled', 'ground', 'trimmed', 'boneless', 'skinless', 'melted', 'softened',\n",
    "    'cooled', 'boiled', 'cooked', 'uncooked', 'raw', 'drained', 'rinsed', 'beaten', 'size'\n",
    "\n",
    "    # Quantity and portion descriptors\n",
    "    'small', 'medium', 'large', 'extra', 'light', 'dark', 'best', 'fresh', 'freshly',\n",
    "    'ripe', 'mini', 'whole', 'big', 'room', 'temperature', 'zero', 'one', 'two', 'three',\n",
    "    'four', 'five', 'six', 'eight', 'ten', 'twelve', 'half', 'third', 'quarter', 'dozen',\n",
    "    'thousand', 'bite'\n",
    "\n",
    "    # Filler or generic stopwords\n",
    "    'plus', 'with', 'without', 'into', 'about', 'of', 'the', 'to', 'for', 'in', 'from',\n",
    "    'as', 'and', 'or', 'on', 'your', 'if', 'such', 'you', 'use', 'may'\n",
    "})\n",
    "\n",
    "def preprocess_ingredients(ingredients):\n",
    "    try:\n",
    "        if isinstance(ingredients, str):\n",
    "            ingredients_list = ast.literal_eval(ingredients) if ingredients.startswith(\"[\") else [ingredients]\n",
    "        elif isinstance(ingredients, list):\n",
    "            ingredients_list = ingredients\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "        cleaned_ingredients = set()\n",
    "\n",
    "        for ing in ingredients_list:\n",
    "            ing = re.sub(r'\\(.*?\\)', '', str(ing)).lower() \n",
    "            ing = re.sub(r'[^a-z\\s]', '', ing)  \n",
    "            tokens = word_tokenize(ing)\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in custom_stopwords and len(token) > 1]\n",
    "            if tokens:\n",
    "                phrase = \" \".join(tokens)\n",
    "                if \"oil\" not in phrase and \"salt\" not in phrase and \"water\" not in phrase:\n",
    "                    cleaned_ingredients.add(phrase)\n",
    "\n",
    "        return \", \".join(sorted(cleaned_ingredients))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_user_ingredients(user_input):\n",
    "    ingredients = user_input.split(',')\n",
    "    ingredients_str = str([ing.strip() for ing in ingredients])\n",
    "    return preprocess_ingredients(ingredients_str)\n",
    "\n",
    "df_combined = pd.read_csv(\"../processed_dataset/combined_recipes_cleaned.csv\") \n",
    "\n",
    "df_combined['ingredients_clean'] = df_combined['ingredients'].apply(preprocess_ingredients)\n",
    "\n",
    "print(df_combined[['ingredients', 'ingredients_clean']].head())\n",
    "\n",
    "df_combined.to_csv('../processed_dataset/combined_recipes_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c312c-83d5-4f01-ac74-9baad54fd0ad",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- TF-IDF + Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Train-Test Split\n",
    "train_df, test_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. TF-IDF Vectorization with Bigrams\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "train_vectors = vectorizer.fit_transform(train_df['ingredients_clean'])\n",
    "\n",
    "# 3. Nearest Neighbors Model (Cosine Similarity)\n",
    "nn = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "nn.fit(train_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e047a78-0b43-4d29-ad62-e330ba6c7e29",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "- Train NLP-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_recipes(user_input, vectorizer, nn_model, train_df):\n",
    "    \"\"\"\n",
    "    Finds the top 10 similar recipes based on user's ingredients input and shows similarity score.\n",
    "    \"\"\"\n",
    "    user_cleaned = preprocess_user_ingredients(user_input)\n",
    "    print(f\"\\nüîç Processed Input: {user_cleaned}\")\n",
    "    \n",
    "    # Vectorize the cleaned user input using the improved TF-IDF vectorizer\n",
    "    user_vector = vectorizer.transform([user_cleaned])\n",
    "    distances, indices = nn_model.kneighbors(user_vector)\n",
    "\n",
    "    print(\"\\nüçΩÔ∏è Top 10 similar recipes:\\n\")\n",
    "    for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "        similarity_score = round((1 - dist) * 100, 2)  # Convert cosine distance to percentage similarity\n",
    "        title = train_df.iloc[idx]['title'] if 'title' in train_df.columns else 'N/A'\n",
    "        ingredients = train_df.iloc[idx]['ingredients']\n",
    "        ingredients_clean = train_df.iloc[idx]['ingredients_clean']\n",
    "\n",
    "        print(f\"{i+1}. Title: {title}\")\n",
    "        print(f\"   Similarity: {similarity_score}%\")\n",
    "        print(f\"   Ingredients: {ingredients}\")\n",
    "        print(f\"   Cleaned Ingredients: {ingredients_clean}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"milk, eggs, sugar, pasta, maida\"\n",
    "find_similar_recipes(user_input, vectorizer, nn, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"onion, tomato, garlic\"\n",
    "find_similar_recipes(user_input, vectorizer, nn, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save vectorizer and nearest neighbors model\n",
    "joblib.dump(vectorizer, \"../models/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(nn, \"../models/nearest_neighbors_model.pkl\")\n",
    "\n",
    "# Optionally save the training DataFrame (with titles + ingredients)\n",
    "train_df.to_csv(\"../processed_dataset/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77078fc7",
   "metadata": {},
   "source": [
    "## Store in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afa14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# Load the preprocessed CSV containing recipes\n",
    "df = pd.read_csv(\"../processed_dataset/combined_recipes_preprocessed.csv\")\n",
    "\n",
    "df = df[['title', 'ingredients', 'instructions']] \n",
    "\n",
    "# Ensure that the database folder exists\n",
    "os.makedirs(\"../database\", exist_ok=True)\n",
    "\n",
    "# Connect to (or create) the SQLite database\n",
    "conn = sqlite3.connect(\"../database/pantrypalette.db\")\n",
    "\n",
    "# Create a table named 'recipes' and populate it with the DataFrame data\n",
    "df.to_sql(\"recipes\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.close()\n",
    "print(\"SQLite database 'pantrypalette.db' has been created and populated.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d622fd28",
   "metadata": {},
   "source": [
    "# Use official Python image as a base\n",
    "FROM python:3.9\n",
    "\n",
    "# Set the working directory inside the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy all files from the current directory to the container\n",
    "COPY . .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Expose the port Streamlit runs on\n",
    "EXPOSE 8501\n",
    "\n",
    "# Command to run the Streamlit app\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85289775-75a5-4914-80e0-bd093f080c37",
   "metadata": {},
   "source": [
    "## UI Development\n",
    "\n",
    "- Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63939d-bbee-483b-a31b-3c16a4cc00ae",
   "metadata": {},
   "source": [
    "## Model Monitoring\n",
    "\n",
    "- PowerBI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7b33c-47eb-403c-af06-833e3e8358ed",
   "metadata": {},
   "source": [
    "<center><h4>Enjoy Your Perfect Recipe! üçΩÔ∏è</h4></center>\n",
    "<center>Designed By: Madhurya Shankar & Sandhya Kilari</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
